# -*- coding: utf-8 -*-
"""livraria_completo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlJc1jhN9za6ZxHKzOULtexaD22IJQlZ

ASR
"""

### **Pipeline Completo - Aula 01: Reconhecimento Autom√°tico de Fala (ASR)**

# **Passo 1: Configura√ß√£o do Ambiente e Upload do √Åudio**
from google.colab import files
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import widgets

# Bot√£o para enviar √°udio
print("Envie o arquivo de √°udio para an√°lise:")
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]  # Obt√©m o nome do arquivo enviado
# **Passo 2: Pr√©-processamento do √Åudio**
def preprocess_audio(audio_path):
    """
    Carrega o √°udio, remove ru√≠do simples (subtra√ß√£o de m√©dia) e normaliza o sinal.
    Retorna o sinal pr√©-processado e a taxa de amostragem.

    Argumentos:
        audio_path (str): Caminho para o arquivo de √°udio a ser processado.

    Retorna:
        tuple: Um sinal de √°udio pr√©-processado e a taxa de amostragem correspondente.
    """
    # Carregar o √°udio
    # `librosa.load` retorna o sinal de √°udio (y) e a taxa de amostragem (sr).
    # Aqui, a taxa de amostragem √© definida como 16kHz para ser compat√≠vel com a maioria dos modelos de ASR.
    print("Carregando o √°udio...")
    y, sr = librosa.load(audio_path, sr=16000)  # Taxa de amostragem padr√£o 16kHz

    # Exibir informa√ß√µes b√°sicas do √°udio carregado
    print(f"Dura√ß√£o do √°udio: {len(y) / sr:.2f} segundos")
    print(f"Taxa de amostragem: {sr} Hz")

    # Salvar o ru√≠do original (m√©dia do sinal)
    ruido = np.mean(y)
    print(f"Ru√≠do detectado (valor m√©dio): {ruido:.4f}")

    # Remo√ß√£o simples de ru√≠do (subtra√ß√£o da m√©dia)
    # Esta etapa reduz o ru√≠do estacion√°rio removendo a m√©dia do sinal.
    # Isso √© especialmente √∫til para eliminar desvios constantes que n√£o representam a fala.
    print("Removendo ru√≠do (subtra√ß√£o da m√©dia)...")
    y_denoised = y - ruido

    # Normaliza√ß√£o
    # A normaliza√ß√£o ajusta os valores de amplitude do sinal para ficarem dentro de uma faixa padr√£o.
    # Isso ajuda a evitar problemas de satura√ß√£o e garante que regi√µes de baixa amplitude sejam destacadas.
    print("Normalizando o √°udio...")
    y_normalized = librosa.util.normalize(y_denoised)
# **Passo 3: Extra√ß√£o de Caracter√≠sticas do √Åudio**
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

def extract_audio_features(audio_signal, sample_rate):
    """
    Extrai caracter√≠sticas importantes do sinal de √°udio, como MFCCs, Mel Spectrogram e caracter√≠sticas temporais.

    Argumentos:
        audio_signal (numpy.ndarray): Sinal de √°udio pr√©-processado.
        sample_rate (int): Taxa de amostragem do sinal.

    Retorna:
        dict: Um dicion√°rio contendo as caracter√≠sticas extra√≠das (MFCC, Mel Spectrogram, RMS).
    """
    print("Extraindo caracter√≠sticas do √°udio...")

    # **1. MFCC (Mel-Frequency Cepstral Coefficients)**
    # Os MFCCs representam a energia do sinal em diferentes bandas de frequ√™ncia baseadas na escala Mel.
    # Isso √© amplamente utilizado em tarefas como reconhecimento de fala.
    #Por padr√£o, 12-13 coeficientes s√£o usados na maioria das aplica√ß√µes de processamento de fala.
    #A taxa de amostragem do √°udio (em Hz): indica quantas amostras por segundo o √°udio possui (16 kHz, sr=16000)
    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=13)
    print(f"MFCCs extra√≠dos: {mfccs.shape[1]} frames com {mfccs.shape[0]} coeficientes")

    # **2. Mel Spectrogram**
    # Representa√ß√£o do espectro de frequ√™ncias na escala Mel (mais pr√≥xima da percep√ß√£o humana).
    # n_fft: N√∫mero de pontos para a Transformada R√°pida de Fourier (FFT).
    # hop_length: Passo (n√∫mero de amostras) entre janelas consecutivas.
    # n_mels: N√∫mero de bandas na escala Mel.

    mel_spectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Converte para dB para melhor visualiza√ß√£o

    # **3. RMS (Root Mean Square)**
    # Calcula a energia m√©dia em cada frame do √°udio.
    rms = librosa.feature.rms(y=audio_signal)

# **Passo 4: Reconhecimento de Fala (Speech-to-Text - STT)**


# Importa√ß√£o de bibliotecas necess√°rias
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torchaudio
import torch
import numpy as np
import wave


# **Fun√ß√£o Whisper: Reconhecimento de fala com Whisper**
def speech_to_text_whisper(audio_path):
    """
    Usa o modelo Whisper para converter √°udio em texto transcrito.
    Retorna o texto transcrito.
    """
    print("Iniciando reconhecimento de fala com Whisper...")

    # Carregar o pipeline do modelo Whisper
    transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-large")

    # Realizar a transcri√ß√£o
    # transcription = transcriber(audio_path)["text"]
    transcription = transcriber(audio_path, return_timestamps=True)["text"]

    print("Texto Transcrito (Whisper):", transcription)
    return transcription

# **Chamando as fun√ß√µes de reconhecimento de fala**
print("Executando transcri√ß√µes com diferentes modelos...")

# Caminho para o arquivo de √°udio
# Use o mesmo arquivo processado anteriormente
transcription_whisper = speech_to_text_whisper(audio_path)

!pip install owlready2 rdflib

from google.colab import files

uploaded = files.upload()
ontology_file = list(uploaded.keys())[0]  # Obt√©m o nome do arquivo enviado
ontology_path = f"/content/{ontology_file}"

print(f"Ontologia carregada: {ontology_file}")

ontology_file

from owlready2 import *

# Carregar a ontologia enviada
onto_path.append("/content/")  # Define o diret√≥rio onde est√° a ontologia
onto = get_ontology(f"file:///{ontology_path}").load()

# Exibir informa√ß√µes b√°sicas
print(f"Nome da Ontologia: {onto.base_iri}")
print(f"Classes na Ontologia: {[cls for cls in onto.classes()]}")

# Listar todas as classes
for cls in onto.classes():
    print(f"Classe: {cls.name}")
#    print(f"  Superclasses: {[superclass.name for superclass in cls.is_a]}")
    print(f"  Subclasses: {[subcls.name for subcls in cls.subclasses()]}")
    print(f"  Inst√¢ncias: {[ind.name for ind in cls.instances()]}")
    print(f"  Propriedades: {[prop.name for prop in cls.get_properties(cls)]}")
    if cls.equivalent_to:
        print(f"Classe: {cls.name} - Equivalente a: {cls.equivalent_to}")
    # hasattr: Verifica se a classe possui um determinado atributo antes de acess√°-lo.
    # Isso evita erros caso o atributo n√£o exista na ontologia.
    if hasattr(cls,"disjoint_with"):
      if cls.disjoint_with:
          print(f"Classe: {cls.name} - Disjunta com: {cls.disjoint_with}")


# Listar todos os indiv√≠duos
for ind in onto.individuals():
    print(f"Indiv√≠duo: {ind.name}")

for livro in onto.search(is_a=onto.Livro):
    print(f"Livro: {livro.name}")
    if hasattr(livro, "temAutor"):
        for autor in livro.temAutor:
            print(f"  - Autor: {autor.name}")

for cls in onto.classes():
    print(f"Classe: {cls.name}")

!pip install transformers accelerate sentencepiece

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Escolha do modelo (Mistral-7B-Instruct √© uma √≥tima op√ß√£o)
model_name = "mistralai/Mistral-7B-Instruct"

# Carregar o modelo e o tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# Criar pipeline para infer√™ncia
generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)

# Fazer uma pergunta
pergunta = "Explique o conceito de aprendizado de m√°quina de forma simples."

# Gerar resposta
resposta = generator(pergunta, max_length=200, do_sample=True, temperature=0.7)
print(resposta[0]['generated_text'])

"""###M√≥dulo 1: Entrada e Pr√©-processamento do Texto

O NLU (Natural Language Understanding) tenta extrair significado e estrutura do texto.
Dois componentes principais:

- Identifica√ß√£o da Inten√ß√£o: O que o usu√°rio quer?

- Extra√ß√£o de Entidades: Quais informa√ß√µes ele forneceu?
"""

!python -m spacy download 'pt_core_news_lg'

import re
import spacy

# Transcri√ß√£o obtida na Aula 01
transcription_whisper = "Qual o livro do autor Jorge Amado a editora Ipsilon publicou?"

# Carregar modelo de NLP do spaCy para portugu√™s
nlp = spacy.load("pt_core_news_lg")

# Fun√ß√£o de pr√©-processamento
def preprocess_text(text):
    text = text.lower().strip()  # Lowercase
    #remove todos os caracteres que n√£o sejam letras, n√∫meros ou espa√ßos da string text
    text = re.sub(r"[^\w\s]", "", text)  # Remover pontua√ß√£o
    return text

# Aplicar NLP
processed_text = preprocess_text(transcription_whisper)
doc = nlp(processed_text)

print("Texto Pr√©-processado:", processed_text)

"""### M√≥dulo 2: Identifica√ß√£o da Inten√ß√£o com BERT
O que √© Identifica√ß√£o de Inten√ß√£o?

Descobrir qual a√ß√£o o usu√°rio quer realizar.
Exemplo:

"Quero uma pizza de pepperoni" ‚Üí Inten√ß√£o: Pedir Pizza

"Quais s√£o os sabores dispon√≠veis?" ‚Üí Inten√ß√£o: Consultar Card√°pio

Modelo: BERT Multilingual
Usaremos BERTimbau, um modelo baseado no BERT treinado para portugu√™s.
"""

from transformers import pipeline

# Criando o pipeline para classifica√ß√£o de inten√ß√£o
#intent_classifier = pipeline("text-classification", model="cartesinus/multilingual_minilm-amazon_massive-intent_eu6_noen")
intent_classifier = pipeline("text-classification", model="neuralmind/bert-base-portuguese-cased")

# Classificar a inten√ß√£o do usu√°rio
intent_result = intent_classifier(processed_text)

# Verificar se a classifica√ß√£o retornou um resultado v√°lido
if intent_result and len(intent_result) > 0:
    intent = intent_result[0]  # Primeiro resultado da lista
    intent_label = intent["label"]
    intent_score = intent["score"]
    print(f"Inten√ß√£o Identificada: {intent_label} (Confian√ßa: {intent_score:.2f})")
else:
    print("Nenhuma inten√ß√£o identificada.")

"""###Treinando um Modelo NLP
Agora treinamos um modelo BERTimbau (neuralmind/bert-base-portuguese-cased) para classificar as inten√ß√µes.

- Carregamos o modelo pr√©-treinado.
- Criamos um dataset compat√≠vel com Hugging Face (datasets.Dataset).
- Fine-tunamos o modelo usando Trainer.
"""

pip install datasets

!pip install datasets transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline
from datasets import Dataset
import torch
import os
os.environ["WANDB_DISABLED"] = "true"

# Defini√ß√£o manual do dataset (frases + inten√ß√µes)
data = [
    {"texto": "Quem √© o autor de Capit√£es de Areia?", "intencao": "consulta_autor"},
    {"texto": "Em que ano foi publicado Capit√£es de Areia?", "intencao": "consulta_ano_publicacao"},
    {"texto": "Quantas p√°ginas tem o livro Capit√£es de Areia?", "intencao": "consulta_paginas"},
    {"texto": "Qual √© a editora do livro Capit√£es de Areia?", "intencao": "consulta_editora"},
    {"texto": "Este livro est√° dispon√≠vel em formato digital?", "intencao": "consulta_formato"},
    {"texto": "Me recomende um livro de fic√ß√£o.", "intencao": "pedido_recomendacao"},
    {"texto": "Quais s√£o os livros de Jorge Amado dispon√≠veis?", "intencao": "consulta_obras_autor"},
    {"texto": "Voc√™s t√™m livros de drama?", "intencao": "consulta_categoria"},
    {"texto": "Qual o peso do livro f√≠sico?", "intencao": "consulta_peso"},
    {"texto": "Em qual prateleira encontro este livro?", "intencao": "consulta_localizacao"}
]

# Criar dataset no formato Hugging Face
dataset = Dataset.from_list(data)

# Mapear os r√≥tulos para IDs num√©ricos
intent_labels = {intent: idx for idx, intent in enumerate(set(d["intencao"] for d in data))}
dataset = dataset.map(lambda example: {"label": intent_labels[example["intencao"]]})

# Dividir dataset em treino e teste (80/20)
dataset = dataset.train_test_split(test_size=0.2)

# Carregar o modelo BERT pr√©-treinado e seu tokenizer
# - `tokenizer`: respons√°vel por converter texto em tokens para o modelo
# - `model`: modelo de classifica√ß√£o baseado em BERT, adaptado para m√∫ltiplas inten√ß√µes
# - `num_labels`: define o n√∫mero de categorias de inten√ß√£o que o modelo deve aprender a classificar
model_name = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(intent_labels))

# Fun√ß√£o para tokenizar os textos de entrada
# - `padding="max_length"`: garante que todas as sequ√™ncias tenham o mesmo tamanho, preenchendo com tokens de padding quando necess√°rio.
# - `truncation=True`: corta textos muito longos para evitar erros de comprimento.
# - `max_length=128`: define um tamanho fixo de 128 tokens para uniformizar as entradas do modelo.
def tokenize_function(examples):
    return tokenizer(examples["texto"], padding="max_length", truncation=True, max_length=128)

dataset = dataset.map(tokenize_function, batched=True)

# Configurar argumentos de treinamento
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Criar e configurar o Trainer para treinamento do modelo
# O Trainer √© uma classe da biblioteca Transformers (Hugging Face) que automatiza
# o processo de treinamento e avalia√ß√£o de modelos de aprendizado profundo.
# - `model`: o modelo BERT carregado para classifica√ß√£o de inten√ß√£o.
# - `args`: par√¢metros de treinamento, como n√∫mero de √©pocas, taxa de aprendizado e estrat√©gias de salvamento.
# - `train_dataset`: conjunto de dados usado para treinar o modelo.
# - `eval_dataset`: conjunto de dados usado para avaliar o desempenho do modelo ap√≥s cada √©poca.
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# Treinar o modelo
trainer.train()

# Salvar modelo treinado
model.save_pretrained("./livraria_intent_model")
tokenizer.save_pretrained("./livraria_intent_model")

# Criar pipeline para infer√™ncia
intent_classifier = pipeline("text-classification", model="./livraria_intent_model", tokenizer="./livraria_intent_model")

# Testar com uma nova frase
test_text = "Voc√™s t√™m livros de drama?"
result = intent_classifier(test_text)[0]

# Mapear o ID da inten√ß√£o para o nome real
#intent_label = {v: k for k, v in intent_labels.items()}[result["label"]]
#print(f"Inten√ß√£o Identificada: {intent_label} (Confian√ßa: {result['score']:.2f})")

# Criar dicion√°rio correto para mapear LABEL_X para as inten√ß√µes
id2label = {v: k for k, v in intent_labels.items()}  # Mapeia {0: 'consulta_ingredientes', 1: 'pedido_recomendacao', ...}

# Obter a inten√ß√£o do modelo
intent_label = id2label[int(result["label"].split("_")[1])]  # Converte "LABEL_4" para 4 e acessa o dicion√°rio

print(f"Inten√ß√£o Identificada: {intent_label} (Confian√ßa: {result['score']:.2f})")

"""### Extra√ß√£o de Entidades (NER - Named Entity Recognition)
O que √© Extra√ß√£o de Entidades?

Descobre informa√ß√µes espec√≠ficas dentro da frase:
Ingredientes, tamanhos, tipos de pizza, etc.
Exemplo:

- "Quero uma pizza de pepperoni com queijo extra."
- Entidade 1: pepperoni ‚Üí Ingrediente
- Entidade 2: queijo extra ‚Üí Modificador

Modelo: BERT para NER (Reconhecimento de Entidades)
Vamos usar BERT para NER e extrair ingredientes, tamanhos e categorias.
"""

ner_model = pipeline("ner", model="pierreguillou/ner-bert-large-cased-pt-lenerbr")

# Extraindo entidades
entities = ner_model(processed_text)

# Fun√ß√£o para combinar tokens fragmentados (ex: ##zza -> pizza)
def merge_entities(entities):
    merged = []
    current_entity = None

    for entity in entities:
        entity_type = entity["entity"]  # Assume entity type is directly in the entity dict

        if entity_type.startswith("O"):  # Ignora entidades sem r√≥tulo
            continue

        word = entity["word"].replace("##", "")  # Remove fragmenta√ß√£o do token

        if current_entity and current_entity["type"] == entity_type:
            current_entity["text"] += word  # Continua formando a palavra
        else:
            if current_entity:
                merged.append(current_entity)  # Adiciona a entidade anterior
            current_entity = {"text": word, "type": entity_type, "score": entity["score"]}

    if current_entity:
        merged.append(current_entity)  # Adiciona a √∫ltima entidade

    return merged

# Processar e juntar os tokens fragmentados
formatted_entities = merge_entities(entities)

# Extraindo entidades
entities = ner_model(processed_text)
print("\nEntidades Identificadas:")
for entity in entities:
    print(f"Texto: {entity['word']} | Tipo: {entity['entity']} | Score: {entity['score']:.2f}")

# üîπ Importa√ß√£o das bibliotecas
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline
from datasets import Dataset, DatasetDict
import os
import torch
import random
import numpy as np


# üîπ **1. Criando o dataset manualmente**
train_data = [
    {"tokens": ["Quero", "ver", "o", "livro", "Capit√£es", "de", "Areia"],
     "ner_tags": [0, 0, 0, 1, 2, 2, 2]},  # Livro e T√≠tulo

    {"tokens": ["O", "livro", "de", "Jorge", "Amado", "est√°", "dispon√≠vel", "na", "editora", "Ipsilon"],
     "ner_tags": [0, 1, 0, 3, 3, 0, 0, 0, 4, 4]},  # Livro, Autor e Editora

    {"tokens": ["Tem", "algum", "livro", "de", "fic√ß√£o", "ou", "drama", "dispon√≠vel"],
     "ner_tags": [0, 0, 1, 0, 5, 0, 5, 0]},  # Livro e Categoria

    {"tokens": ["Quero", "um", "livro", "digital", "em", "formato", "PDF"],
     "ner_tags": [0, 0, 1, 6, 0, 7, 7]},  # Livro, Tipo e Formato

    {"tokens": ["O", "livro", "f√≠sico", "est√°", "na", "prateleira", "cinco"],
     "ner_tags": [0, 1, 6, 0, 0, 8, 8]},  # Livro, Tipo e Localiza√ß√£o

    {"tokens": ["Quantas", "p√°ginas", "tem", "o", "livro", "Capit√£es", "de", "Areia"],
     "ner_tags": [0, 9, 0, 0, 1, 2, 2, 2]},  # P√°ginas, Livro e T√≠tulo
]

# Criar dataset no formato Hugging Face
dataset = DatasetDict({
    "train": Dataset.from_list(train_data),
    "test": Dataset.from_list(train_data[:1])  # Pequeno conjunto de teste
})

# üîπ Mapeamento de r√≥tulos para IDs num√©ricos
labels = {
   "O": 0,
   "B-LIVRO": 1,
   "B-TITULO": 2,
   "B-AUTOR": 3,
   "B-EDITORA": 4,
   "B-CATEGORIA": 5,
   "B-TIPO": 6, # Digital/F√≠sico
   "B-FORMATO": 7, # PDF, EPUB etc
   "B-LOCAL": 8, # Prateleira/Localiza√ß√£o
   "B-ATRIBUTO": 9, # P√°ginas, Peso etc
   "I-LIVRO": 10,
   "I-TITULO": 11,
   "I-AUTOR": 12,
   "I-EDITORA": 13,
   "I-CATEGORIA": 14,
   "I-TIPO": 15,
   "I-FORMATO": 16,
   "I-LOCAL": 17,
   "I-ATRIBUTO": 18
}

#TESTE ROTULOS
all_labels = set()
for example in train_data:
    all_labels.update(example["ner_tags"])

print("R√≥tulos √∫nicos encontrados no dataset:", all_labels)
print("R√≥tulos esperados:", set(labels.values()))


# üîπ Carregar tokenizer
model_name = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# üîπ Fun√ß√£o para Tokenizar e Alinhar os R√≥tulos
def tokenize_and_align_labels(example):
    tokenized_inputs = tokenizer(example["tokens"], truncation=True, padding="max_length",
                                  max_length=64, is_split_into_words=True)

    labels_aligned = []
    word_ids = tokenized_inputs.word_ids()

    previous_word_idx = None  # √çndice da palavra anterior
    for i, word_idx in enumerate(word_ids):
        if word_idx is None:
            labels_aligned.append(-100)  # Ignorar tokens de padding
        elif word_idx != previous_word_idx:
            labels_aligned.append(example["ner_tags"][word_idx])  # Atribuir r√≥tulo √† primeira parte da palavra
        else:
            labels_aligned.append(-100)  # Ignorar tokens quebrados (subwords)

        previous_word_idx = word_idx

    tokenized_inputs["labels"] = labels_aligned
    return tokenized_inputs


# **üîπ 3. Aplicar tokeniza√ß√£o e alinhamento**
tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)

# **4. Treinamento do Modelo**
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels))

training_args = TrainingArguments(
    output_dir="./ner_livraria_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    gradient_accumulation_steps=2,
    fp16=False,  # Desativar fp16 para evitar problemas na GPU
    no_cuda=True  # Rodar apenas na CPU
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

print("üîπ Treinando o modelo NER...")
trainer.train()

# **5. Salvando o Modelo Treinado**
model.save_pretrained("./ner_livraria_model")
tokenizer.save_pretrained("./ner_livraria_model")

# **6. Infer√™ncia com o Modelo Treinado**
print("üîπ Modelo Treinado! Testando infer√™ncia...")

# Criar pipeline para infer√™ncia
ner_pipeline = pipeline("ner", model="./ner_livraria_model", tokenizer="./ner_livraria_model")

# **7. Testar com uma frase nova**
#processed_text = "Gostaria de uma livraria quatro queijos com borda recheada e massa fina"
entities = ner_pipeline(processed_text)

# üîπ Mapeamento dos r√≥tulos num√©ricos para nomes das entidades
id2label = {
   "LABEL_0": "O",
   "LABEL_1": "B-LIVRO",
   "LABEL_2": "B-TITULO",
   "LABEL_3": "B-AUTOR",
   "LABEL_4": "B-EDITORA",
   "LABEL_5": "B-CATEGORIA",
   "LABEL_6": "B-TIPO",
   "LABEL_7": "B-FORMATO",
   "LABEL_8": "B-LOCAL",
   "LABEL_9": "B-ATRIBUTO",
   "LABEL_10": "I-LIVRO",
   "LABEL_11": "I-TITULO",
   "LABEL_12": "I-AUTOR",
   "LABEL_13": "I-EDITORA",
   "LABEL_14": "I-CATEGORIA",
   "LABEL_15": "I-TIPO",
   "LABEL_16": "I-FORMATO",
   "LABEL_17": "I-LOCAL",
   "LABEL_18": "I-ATRIBUTO"
}

# üîπ Fun√ß√£o para combinar tokens fragmentados (ex: ##zza -> pizza)
def merge_entities(entities):
    merged = []
    current_entity = None

    for entity in entities:
        entity_type = id2label.get(entity["entity"], "DESCONHECIDO")

        if entity_type == "O":  # Ignora entidades sem r√≥tulo
            continue

        word = entity["word"].replace("##", "")  # Remove fragmenta√ß√£o do token

        if current_entity and current_entity["type"] == entity_type:
            current_entity["text"] += word  # Continua formando a palavra
        else:
            if current_entity:
                merged.append(current_entity)  # Adiciona a entidade anterior
            current_entity = {"text": word, "type": entity_type, "score": entity["score"]}

    if current_entity:
        merged.append(current_entity)  # Adiciona a √∫ltima entidade

    return merged

# üîπ Processar e formatar a sa√≠da
formatted_entities = merge_entities(entities)

# üîπ Exibir entidades processadas
print("\nüîπ **Entidades Identificadas:**")
for entity in formatted_entities:
    print(f"üü¢ Texto: {entity['text']} | Tipo: {entity['type']} | Confian√ßa: {entity['score']:.2f}")

"""### A partir da Inten√ß√£o e Entidades --> Conectar com a Ontologia"""

#A ideia √© fazer a consulta (inten√ß√£o) usando os par√¢metros (entidades) informadas...
#Tamb√©m, aprimorar com o NLG abaixo onde a ontologia poder√° fornecer o contexto

"""###M√≥dulo 5: Gera√ß√£o de Resposta com IA Generativa (NLG)
O que √© NLG?

Transforma informa√ß√µes estruturadas em frases naturais.
Exemplo:
Input: Pizza de pepperoni dispon√≠vel nos tamanhos m√©dio e grande.
Output: "√ìtima escolha! A pizza de pepperoni est√° dispon√≠vel nos tamanhos m√©dio e grande."


"""

from transformers import pipeline

# üîπ Criar pipeline de Question Answering (QA)
qa_pipeline = pipeline("question-answering", model="pierreguillou/bert-base-cased-squad-v1.1-portuguese")

# üîπ Contexto com informa√ß√µes sobre livraria e categorias
contexto = """
O livro Capit√£es da Areia √© um romance do autor Jorge Amado que pertence √† categoria de literatura brasileira.
A editora Ipsilon que publica e disponibiliza os seus livros em uma livraria.
A livraria possui diferentes se√ß√µes como literatura brasileira, literatura estrangeira, infantil e autoajuda.
Os livros de romance podem ser encontrados tanto na se√ß√£o de literatura brasileira quanto na literatura estrangeira.
A se√ß√£o de literatura brasileira cont√©m obras de autores como Jorge Amado, Machado de Assis e Clarice Lispector.
A livraria oferece livros em diferentes formatos como capa dura, brochura e edi√ß√£o de bolso.
Os livros podem ser classificados por g√™neros como romance, poesia, biografia e fic√ß√£o cient√≠fica.
"""

def gerar_resposta(intent, entities):
    if intent == "consulta_editora":
        pergunta = "Qual √© o nome da editora?"
    elif intent == "consulta_autor":
        pergunta = f"Quem √© o autor de {entities}?"
    elif intent == "consulta_categoria":
        pergunta = f"Qual √© a categoria do livro {entities}?"
    else:
        return "Desculpe, n√£o entendi a solicita√ß√£o."

    print(f"Pergunta gerada: {pergunta}")
    resposta = qa_pipeline(question=pergunta, context=contexto)
    print(f"Resposta: {resposta['answer']}")  # Adicione esta linha
    return resposta["answer"]

# Teste com diferentes inten√ß√µes
intencoes = ["consulta_autor", "consulta_categoria", "consulta_editora"]

for intencao in intencoes:
    print(f"\n=== Testando {intencao} ===")
    resposta = gerar_resposta(intencao, "Capit√£es da Areia")
    print(f"Resposta final: {resposta}")

!pip install ctransformers

from ctransformers import AutoModelForCausalLM
from typing import Dict, List, Optional
import torch

class BibliotecaLLM:
    def __init__(self):
        """
        Inicializa o componente LLM para a biblioteca usando LLaMA 2.
        Usa o modelo quantizado para economia de mem√≥ria.
        """
        # Carrega o modelo LLaMA 2 quantizado
        self.model = AutoModelForCausalLM.from_pretrained(
            'TheBloke/Llama-2-7B-Chat-GGML',
            model_type='llama',
            gpu_layers=0  # Usar apenas CPU para compatibilidade
        )

        self.system_prompt = """
        Voc√™ √© um assistente especializado em biblioteca que ajuda a responder perguntas sobre livros.
        Use as informa√ß√µes da ontologia para fornecer respostas precisas e naturais.
        Mantenha as respostas concisas e relevantes ao contexto da pergunta.
        """

    def prepare_context(self,
                       transcription: str,
                       intent: str,
                       entities: List[Dict],
                       ontology_data: Dict) -> str:
        """
        Prepara o contexto para o LLM com base nas informa√ß√µes extra√≠das.
        """
        context = f"""
        Pergunta do usu√°rio: {transcription}
        Inten√ß√£o identificada: {intent}

        Entidades encontradas:
        {self._format_entities(entities)}

        Informa√ß√µes da ontologia:
        {self._format_ontology(ontology_data)}

        Responda de forma natural e concisa.
        """
        return context

    def generate_response(self,
                         context: str,
                         max_tokens: int = 150) -> str:
        """
        Gera uma resposta usando o modelo LLaMA 2.
        """
        try:
            # Combina o prompt do sistema com o contexto
            full_prompt = f"{self.system_prompt}\n\n{context}"

            # Gera a resposta
            response = self.model(
                full_prompt,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.95,
                repetition_penalty=1.15
            )

            return response

        except Exception as e:
            print(f"Erro ao gerar resposta: {e}")
            return "Desculpe, n√£o foi poss√≠vel gerar uma resposta no momento."

    def _format_entities(self, entities: List[Dict]) -> str:
        """Formata as entidades para inclus√£o no contexto."""
        formatted = []
        for entity in entities:
            formatted.append(f"- {entity['text']} ({entity['type']})")
        return "\n".join(formatted)

    def _format_ontology(self, ontology_data: Dict) -> str:
        """Formata os dados da ontologia para inclus√£o no contexto."""
        formatted = []
        for key, value in ontology_data.items():
            if isinstance(value, list):
                formatted.append(f"{key}: {', '.join(value)}")
            else:
                formatted.append(f"{key}: {value}")
        return "\n".join(formatted)

# Exemplo de uso
if __name__ == "__main__":
    # Inicializa√ß√£o (n√£o precisa de API key)
    llm = BibliotecaLLM()

    # Exemplo de dados
    transcription = "Qual o livro do autor Jorge Amado a editora Ipsilon publicou?"
    intent = "consulta_obra"
    entities = [
        {"text": "Jorge Amado", "type": "AUTOR"},
        {"text": "Ipsilon", "type": "EDITORA"}
    ]
    ontology_data = {
        "autor": "Jorge Amado",
        "obras": ["Capit√£es da Areia"],
        "editora": "Ipsilon"
    }

    # Preparar contexto e gerar resposta
    context = llm.prepare_context(transcription, intent, entities, ontology_data)
    response = llm.generate_response(context)
    print(f"Resposta: {response}")

"""TTS

###Modelo Baseado em Aprendizagem de M√°quina
"""

# Instala a biblioteca gTTS (Google Text-to-Speech)
!pip install gtts

# Importa as bibliotecas necess√°rias
from gtts import gTTS  # Google TTS para convers√£o de texto em fala
from IPython.display import Audio  # Para reproduzir o √°udio no Jupyter Notebook

# Define o texto que ser√° sintetizado
text = response

# Cria o objeto TTS (define a l√≠ngua do texto como portugu√™s)
tts = gTTS(text, lang="pt")

# Salva o √°udio gerado em um arquivo
output_file = "output_ml.mp3"  # Nome do arquivo de sa√≠da
tts.save(output_file)  # Salva o √°udio gerado como MP3

print(f"√Åudio gerado e salvo: '{output_file}'!")

from IPython.display import Audio


# Reproduzir √°udio
Audio(output_file)