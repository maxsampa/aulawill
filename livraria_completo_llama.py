# -*- coding: utf-8 -*-
"""livraria_completo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlJc1jhN9za6ZxHKzOULtexaD22IJQlZ

ASR
"""

### **Pipeline Completo - Aula 01: Reconhecimento Automático de Fala (ASR)**

# **Passo 1: Configuração do Ambiente e Upload do Áudio**
from google.colab import files
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import widgets

# Botão para enviar áudio
print("Envie o arquivo de áudio para análise:")
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]  # Obtém o nome do arquivo enviado
# **Passo 2: Pré-processamento do Áudio**
def preprocess_audio(audio_path):
    """
    Carrega o áudio, remove ruído simples (subtração de média) e normaliza o sinal.
    Retorna o sinal pré-processado e a taxa de amostragem.

    Argumentos:
        audio_path (str): Caminho para o arquivo de áudio a ser processado.

    Retorna:
        tuple: Um sinal de áudio pré-processado e a taxa de amostragem correspondente.
    """
    # Carregar o áudio
    # `librosa.load` retorna o sinal de áudio (y) e a taxa de amostragem (sr).
    # Aqui, a taxa de amostragem é definida como 16kHz para ser compatível com a maioria dos modelos de ASR.
    print("Carregando o áudio...")
    y, sr = librosa.load(audio_path, sr=16000)  # Taxa de amostragem padrão 16kHz

    # Exibir informações básicas do áudio carregado
    print(f"Duração do áudio: {len(y) / sr:.2f} segundos")
    print(f"Taxa de amostragem: {sr} Hz")

    # Salvar o ruído original (média do sinal)
    ruido = np.mean(y)
    print(f"Ruído detectado (valor médio): {ruido:.4f}")

    # Remoção simples de ruído (subtração da média)
    # Esta etapa reduz o ruído estacionário removendo a média do sinal.
    # Isso é especialmente útil para eliminar desvios constantes que não representam a fala.
    print("Removendo ruído (subtração da média)...")
    y_denoised = y - ruido

    # Normalização
    # A normalização ajusta os valores de amplitude do sinal para ficarem dentro de uma faixa padrão.
    # Isso ajuda a evitar problemas de saturação e garante que regiões de baixa amplitude sejam destacadas.
    print("Normalizando o áudio...")
    y_normalized = librosa.util.normalize(y_denoised)
# **Passo 3: Extração de Características do Áudio**
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

def extract_audio_features(audio_signal, sample_rate):
    """
    Extrai características importantes do sinal de áudio, como MFCCs, Mel Spectrogram e características temporais.

    Argumentos:
        audio_signal (numpy.ndarray): Sinal de áudio pré-processado.
        sample_rate (int): Taxa de amostragem do sinal.

    Retorna:
        dict: Um dicionário contendo as características extraídas (MFCC, Mel Spectrogram, RMS).
    """
    print("Extraindo características do áudio...")

    # **1. MFCC (Mel-Frequency Cepstral Coefficients)**
    # Os MFCCs representam a energia do sinal em diferentes bandas de frequência baseadas na escala Mel.
    # Isso é amplamente utilizado em tarefas como reconhecimento de fala.
    #Por padrão, 12-13 coeficientes são usados na maioria das aplicações de processamento de fala.
    #A taxa de amostragem do áudio (em Hz): indica quantas amostras por segundo o áudio possui (16 kHz, sr=16000)
    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=13)
    print(f"MFCCs extraídos: {mfccs.shape[1]} frames com {mfccs.shape[0]} coeficientes")

    # **2. Mel Spectrogram**
    # Representação do espectro de frequências na escala Mel (mais próxima da percepção humana).
    # n_fft: Número de pontos para a Transformada Rápida de Fourier (FFT).
    # hop_length: Passo (número de amostras) entre janelas consecutivas.
    # n_mels: Número de bandas na escala Mel.

    mel_spectrogram = librosa.feature.melspectrogram(y=audio_signal, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)  # Converte para dB para melhor visualização

    # **3. RMS (Root Mean Square)**
    # Calcula a energia média em cada frame do áudio.
    rms = librosa.feature.rms(y=audio_signal)

# **Passo 4: Reconhecimento de Fala (Speech-to-Text - STT)**


# Importação de bibliotecas necessárias
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import torchaudio
import torch
import numpy as np
import wave


# **Função Whisper: Reconhecimento de fala com Whisper**
def speech_to_text_whisper(audio_path):
    """
    Usa o modelo Whisper para converter áudio em texto transcrito.
    Retorna o texto transcrito.
    """
    print("Iniciando reconhecimento de fala com Whisper...")

    # Carregar o pipeline do modelo Whisper
    transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-large")

    # Realizar a transcrição
    # transcription = transcriber(audio_path)["text"]
    transcription = transcriber(audio_path, return_timestamps=True)["text"]

    print("Texto Transcrito (Whisper):", transcription)
    return transcription

# **Chamando as funções de reconhecimento de fala**
print("Executando transcrições com diferentes modelos...")

# Caminho para o arquivo de áudio
# Use o mesmo arquivo processado anteriormente
transcription_whisper = speech_to_text_whisper(audio_path)

!pip install owlready2 rdflib

from google.colab import files

uploaded = files.upload()
ontology_file = list(uploaded.keys())[0]  # Obtém o nome do arquivo enviado
ontology_path = f"/content/{ontology_file}"

print(f"Ontologia carregada: {ontology_file}")

ontology_file

from owlready2 import *

# Carregar a ontologia enviada
onto_path.append("/content/")  # Define o diretório onde está a ontologia
onto = get_ontology(f"file:///{ontology_path}").load()

# Exibir informações básicas
print(f"Nome da Ontologia: {onto.base_iri}")
print(f"Classes na Ontologia: {[cls for cls in onto.classes()]}")

# Listar todas as classes
for cls in onto.classes():
    print(f"Classe: {cls.name}")
#    print(f"  Superclasses: {[superclass.name for superclass in cls.is_a]}")
    print(f"  Subclasses: {[subcls.name for subcls in cls.subclasses()]}")
    print(f"  Instâncias: {[ind.name for ind in cls.instances()]}")
    print(f"  Propriedades: {[prop.name for prop in cls.get_properties(cls)]}")
    if cls.equivalent_to:
        print(f"Classe: {cls.name} - Equivalente a: {cls.equivalent_to}")
    # hasattr: Verifica se a classe possui um determinado atributo antes de acessá-lo.
    # Isso evita erros caso o atributo não exista na ontologia.
    if hasattr(cls,"disjoint_with"):
      if cls.disjoint_with:
          print(f"Classe: {cls.name} - Disjunta com: {cls.disjoint_with}")


# Listar todos os indivíduos
for ind in onto.individuals():
    print(f"Indivíduo: {ind.name}")

for livro in onto.search(is_a=onto.Livro):
    print(f"Livro: {livro.name}")
    if hasattr(livro, "temAutor"):
        for autor in livro.temAutor:
            print(f"  - Autor: {autor.name}")

for cls in onto.classes():
    print(f"Classe: {cls.name}")

!pip install transformers accelerate sentencepiece

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Escolha do modelo (Mistral-7B-Instruct é uma ótima opção)
model_name = "mistralai/Mistral-7B-Instruct"

# Carregar o modelo e o tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# Criar pipeline para inferência
generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)

# Fazer uma pergunta
pergunta = "Explique o conceito de aprendizado de máquina de forma simples."

# Gerar resposta
resposta = generator(pergunta, max_length=200, do_sample=True, temperature=0.7)
print(resposta[0]['generated_text'])

"""###Módulo 1: Entrada e Pré-processamento do Texto

O NLU (Natural Language Understanding) tenta extrair significado e estrutura do texto.
Dois componentes principais:

- Identificação da Intenção: O que o usuário quer?

- Extração de Entidades: Quais informações ele forneceu?
"""

!python -m spacy download 'pt_core_news_lg'

import re
import spacy

# Transcrição obtida na Aula 01
transcription_whisper = "Qual o livro do autor Jorge Amado a editora Ipsilon publicou?"

# Carregar modelo de NLP do spaCy para português
nlp = spacy.load("pt_core_news_lg")

# Função de pré-processamento
def preprocess_text(text):
    text = text.lower().strip()  # Lowercase
    #remove todos os caracteres que não sejam letras, números ou espaços da string text
    text = re.sub(r"[^\w\s]", "", text)  # Remover pontuação
    return text

# Aplicar NLP
processed_text = preprocess_text(transcription_whisper)
doc = nlp(processed_text)

print("Texto Pré-processado:", processed_text)

"""### Módulo 2: Identificação da Intenção com BERT
O que é Identificação de Intenção?

Descobrir qual ação o usuário quer realizar.
Exemplo:

"Quero uma pizza de pepperoni" → Intenção: Pedir Pizza

"Quais são os sabores disponíveis?" → Intenção: Consultar Cardápio

Modelo: BERT Multilingual
Usaremos BERTimbau, um modelo baseado no BERT treinado para português.
"""

from transformers import pipeline

# Criando o pipeline para classificação de intenção
#intent_classifier = pipeline("text-classification", model="cartesinus/multilingual_minilm-amazon_massive-intent_eu6_noen")
intent_classifier = pipeline("text-classification", model="neuralmind/bert-base-portuguese-cased")

# Classificar a intenção do usuário
intent_result = intent_classifier(processed_text)

# Verificar se a classificação retornou um resultado válido
if intent_result and len(intent_result) > 0:
    intent = intent_result[0]  # Primeiro resultado da lista
    intent_label = intent["label"]
    intent_score = intent["score"]
    print(f"Intenção Identificada: {intent_label} (Confiança: {intent_score:.2f})")
else:
    print("Nenhuma intenção identificada.")

"""###Treinando um Modelo NLP
Agora treinamos um modelo BERTimbau (neuralmind/bert-base-portuguese-cased) para classificar as intenções.

- Carregamos o modelo pré-treinado.
- Criamos um dataset compatível com Hugging Face (datasets.Dataset).
- Fine-tunamos o modelo usando Trainer.
"""

pip install datasets

!pip install datasets transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline
from datasets import Dataset
import torch
import os
os.environ["WANDB_DISABLED"] = "true"

# Definição manual do dataset (frases + intenções)
data = [
    {"texto": "Quem é o autor de Capitães de Areia?", "intencao": "consulta_autor"},
    {"texto": "Em que ano foi publicado Capitães de Areia?", "intencao": "consulta_ano_publicacao"},
    {"texto": "Quantas páginas tem o livro Capitães de Areia?", "intencao": "consulta_paginas"},
    {"texto": "Qual é a editora do livro Capitães de Areia?", "intencao": "consulta_editora"},
    {"texto": "Este livro está disponível em formato digital?", "intencao": "consulta_formato"},
    {"texto": "Me recomende um livro de ficção.", "intencao": "pedido_recomendacao"},
    {"texto": "Quais são os livros de Jorge Amado disponíveis?", "intencao": "consulta_obras_autor"},
    {"texto": "Vocês têm livros de drama?", "intencao": "consulta_categoria"},
    {"texto": "Qual o peso do livro físico?", "intencao": "consulta_peso"},
    {"texto": "Em qual prateleira encontro este livro?", "intencao": "consulta_localizacao"}
]

# Criar dataset no formato Hugging Face
dataset = Dataset.from_list(data)

# Mapear os rótulos para IDs numéricos
intent_labels = {intent: idx for idx, intent in enumerate(set(d["intencao"] for d in data))}
dataset = dataset.map(lambda example: {"label": intent_labels[example["intencao"]]})

# Dividir dataset em treino e teste (80/20)
dataset = dataset.train_test_split(test_size=0.2)

# Carregar o modelo BERT pré-treinado e seu tokenizer
# - `tokenizer`: responsável por converter texto em tokens para o modelo
# - `model`: modelo de classificação baseado em BERT, adaptado para múltiplas intenções
# - `num_labels`: define o número de categorias de intenção que o modelo deve aprender a classificar
model_name = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(intent_labels))

# Função para tokenizar os textos de entrada
# - `padding="max_length"`: garante que todas as sequências tenham o mesmo tamanho, preenchendo com tokens de padding quando necessário.
# - `truncation=True`: corta textos muito longos para evitar erros de comprimento.
# - `max_length=128`: define um tamanho fixo de 128 tokens para uniformizar as entradas do modelo.
def tokenize_function(examples):
    return tokenizer(examples["texto"], padding="max_length", truncation=True, max_length=128)

dataset = dataset.map(tokenize_function, batched=True)

# Configurar argumentos de treinamento
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Criar e configurar o Trainer para treinamento do modelo
# O Trainer é uma classe da biblioteca Transformers (Hugging Face) que automatiza
# o processo de treinamento e avaliação de modelos de aprendizado profundo.
# - `model`: o modelo BERT carregado para classificação de intenção.
# - `args`: parâmetros de treinamento, como número de épocas, taxa de aprendizado e estratégias de salvamento.
# - `train_dataset`: conjunto de dados usado para treinar o modelo.
# - `eval_dataset`: conjunto de dados usado para avaliar o desempenho do modelo após cada época.
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# Treinar o modelo
trainer.train()

# Salvar modelo treinado
model.save_pretrained("./livraria_intent_model")
tokenizer.save_pretrained("./livraria_intent_model")

# Criar pipeline para inferência
intent_classifier = pipeline("text-classification", model="./livraria_intent_model", tokenizer="./livraria_intent_model")

# Testar com uma nova frase
test_text = "Vocês têm livros de drama?"
result = intent_classifier(test_text)[0]

# Mapear o ID da intenção para o nome real
#intent_label = {v: k for k, v in intent_labels.items()}[result["label"]]
#print(f"Intenção Identificada: {intent_label} (Confiança: {result['score']:.2f})")

# Criar dicionário correto para mapear LABEL_X para as intenções
id2label = {v: k for k, v in intent_labels.items()}  # Mapeia {0: 'consulta_ingredientes', 1: 'pedido_recomendacao', ...}

# Obter a intenção do modelo
intent_label = id2label[int(result["label"].split("_")[1])]  # Converte "LABEL_4" para 4 e acessa o dicionário

print(f"Intenção Identificada: {intent_label} (Confiança: {result['score']:.2f})")

"""### Extração de Entidades (NER - Named Entity Recognition)
O que é Extração de Entidades?

Descobre informações específicas dentro da frase:
Ingredientes, tamanhos, tipos de pizza, etc.
Exemplo:

- "Quero uma pizza de pepperoni com queijo extra."
- Entidade 1: pepperoni → Ingrediente
- Entidade 2: queijo extra → Modificador

Modelo: BERT para NER (Reconhecimento de Entidades)
Vamos usar BERT para NER e extrair ingredientes, tamanhos e categorias.
"""

ner_model = pipeline("ner", model="pierreguillou/ner-bert-large-cased-pt-lenerbr")

# Extraindo entidades
entities = ner_model(processed_text)

# Função para combinar tokens fragmentados (ex: ##zza -> pizza)
def merge_entities(entities):
    merged = []
    current_entity = None

    for entity in entities:
        entity_type = entity["entity"]  # Assume entity type is directly in the entity dict

        if entity_type.startswith("O"):  # Ignora entidades sem rótulo
            continue

        word = entity["word"].replace("##", "")  # Remove fragmentação do token

        if current_entity and current_entity["type"] == entity_type:
            current_entity["text"] += word  # Continua formando a palavra
        else:
            if current_entity:
                merged.append(current_entity)  # Adiciona a entidade anterior
            current_entity = {"text": word, "type": entity_type, "score": entity["score"]}

    if current_entity:
        merged.append(current_entity)  # Adiciona a última entidade

    return merged

# Processar e juntar os tokens fragmentados
formatted_entities = merge_entities(entities)

# Extraindo entidades
entities = ner_model(processed_text)
print("\nEntidades Identificadas:")
for entity in entities:
    print(f"Texto: {entity['word']} | Tipo: {entity['entity']} | Score: {entity['score']:.2f}")

# 🔹 Importação das bibliotecas
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline
from datasets import Dataset, DatasetDict
import os
import torch
import random
import numpy as np


# 🔹 **1. Criando o dataset manualmente**
train_data = [
    {"tokens": ["Quero", "ver", "o", "livro", "Capitães", "de", "Areia"],
     "ner_tags": [0, 0, 0, 1, 2, 2, 2]},  # Livro e Título

    {"tokens": ["O", "livro", "de", "Jorge", "Amado", "está", "disponível", "na", "editora", "Ipsilon"],
     "ner_tags": [0, 1, 0, 3, 3, 0, 0, 0, 4, 4]},  # Livro, Autor e Editora

    {"tokens": ["Tem", "algum", "livro", "de", "ficção", "ou", "drama", "disponível"],
     "ner_tags": [0, 0, 1, 0, 5, 0, 5, 0]},  # Livro e Categoria

    {"tokens": ["Quero", "um", "livro", "digital", "em", "formato", "PDF"],
     "ner_tags": [0, 0, 1, 6, 0, 7, 7]},  # Livro, Tipo e Formato

    {"tokens": ["O", "livro", "físico", "está", "na", "prateleira", "cinco"],
     "ner_tags": [0, 1, 6, 0, 0, 8, 8]},  # Livro, Tipo e Localização

    {"tokens": ["Quantas", "páginas", "tem", "o", "livro", "Capitães", "de", "Areia"],
     "ner_tags": [0, 9, 0, 0, 1, 2, 2, 2]},  # Páginas, Livro e Título
]

# Criar dataset no formato Hugging Face
dataset = DatasetDict({
    "train": Dataset.from_list(train_data),
    "test": Dataset.from_list(train_data[:1])  # Pequeno conjunto de teste
})

# 🔹 Mapeamento de rótulos para IDs numéricos
labels = {
   "O": 0,
   "B-LIVRO": 1,
   "B-TITULO": 2,
   "B-AUTOR": 3,
   "B-EDITORA": 4,
   "B-CATEGORIA": 5,
   "B-TIPO": 6, # Digital/Físico
   "B-FORMATO": 7, # PDF, EPUB etc
   "B-LOCAL": 8, # Prateleira/Localização
   "B-ATRIBUTO": 9, # Páginas, Peso etc
   "I-LIVRO": 10,
   "I-TITULO": 11,
   "I-AUTOR": 12,
   "I-EDITORA": 13,
   "I-CATEGORIA": 14,
   "I-TIPO": 15,
   "I-FORMATO": 16,
   "I-LOCAL": 17,
   "I-ATRIBUTO": 18
}

#TESTE ROTULOS
all_labels = set()
for example in train_data:
    all_labels.update(example["ner_tags"])

print("Rótulos únicos encontrados no dataset:", all_labels)
print("Rótulos esperados:", set(labels.values()))


# 🔹 Carregar tokenizer
model_name = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 🔹 Função para Tokenizar e Alinhar os Rótulos
def tokenize_and_align_labels(example):
    tokenized_inputs = tokenizer(example["tokens"], truncation=True, padding="max_length",
                                  max_length=64, is_split_into_words=True)

    labels_aligned = []
    word_ids = tokenized_inputs.word_ids()

    previous_word_idx = None  # Índice da palavra anterior
    for i, word_idx in enumerate(word_ids):
        if word_idx is None:
            labels_aligned.append(-100)  # Ignorar tokens de padding
        elif word_idx != previous_word_idx:
            labels_aligned.append(example["ner_tags"][word_idx])  # Atribuir rótulo à primeira parte da palavra
        else:
            labels_aligned.append(-100)  # Ignorar tokens quebrados (subwords)

        previous_word_idx = word_idx

    tokenized_inputs["labels"] = labels_aligned
    return tokenized_inputs


# **🔹 3. Aplicar tokenização e alinhamento**
tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)

# **4. Treinamento do Modelo**
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels))

training_args = TrainingArguments(
    output_dir="./ner_livraria_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    gradient_accumulation_steps=2,
    fp16=False,  # Desativar fp16 para evitar problemas na GPU
    no_cuda=True  # Rodar apenas na CPU
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

print("🔹 Treinando o modelo NER...")
trainer.train()

# **5. Salvando o Modelo Treinado**
model.save_pretrained("./ner_livraria_model")
tokenizer.save_pretrained("./ner_livraria_model")

# **6. Inferência com o Modelo Treinado**
print("🔹 Modelo Treinado! Testando inferência...")

# Criar pipeline para inferência
ner_pipeline = pipeline("ner", model="./ner_livraria_model", tokenizer="./ner_livraria_model")

# **7. Testar com uma frase nova**
#processed_text = "Gostaria de uma livraria quatro queijos com borda recheada e massa fina"
entities = ner_pipeline(processed_text)

# 🔹 Mapeamento dos rótulos numéricos para nomes das entidades
id2label = {
   "LABEL_0": "O",
   "LABEL_1": "B-LIVRO",
   "LABEL_2": "B-TITULO",
   "LABEL_3": "B-AUTOR",
   "LABEL_4": "B-EDITORA",
   "LABEL_5": "B-CATEGORIA",
   "LABEL_6": "B-TIPO",
   "LABEL_7": "B-FORMATO",
   "LABEL_8": "B-LOCAL",
   "LABEL_9": "B-ATRIBUTO",
   "LABEL_10": "I-LIVRO",
   "LABEL_11": "I-TITULO",
   "LABEL_12": "I-AUTOR",
   "LABEL_13": "I-EDITORA",
   "LABEL_14": "I-CATEGORIA",
   "LABEL_15": "I-TIPO",
   "LABEL_16": "I-FORMATO",
   "LABEL_17": "I-LOCAL",
   "LABEL_18": "I-ATRIBUTO"
}

# 🔹 Função para combinar tokens fragmentados (ex: ##zza -> pizza)
def merge_entities(entities):
    merged = []
    current_entity = None

    for entity in entities:
        entity_type = id2label.get(entity["entity"], "DESCONHECIDO")

        if entity_type == "O":  # Ignora entidades sem rótulo
            continue

        word = entity["word"].replace("##", "")  # Remove fragmentação do token

        if current_entity and current_entity["type"] == entity_type:
            current_entity["text"] += word  # Continua formando a palavra
        else:
            if current_entity:
                merged.append(current_entity)  # Adiciona a entidade anterior
            current_entity = {"text": word, "type": entity_type, "score": entity["score"]}

    if current_entity:
        merged.append(current_entity)  # Adiciona a última entidade

    return merged

# 🔹 Processar e formatar a saída
formatted_entities = merge_entities(entities)

# 🔹 Exibir entidades processadas
print("\n🔹 **Entidades Identificadas:**")
for entity in formatted_entities:
    print(f"🟢 Texto: {entity['text']} | Tipo: {entity['type']} | Confiança: {entity['score']:.2f}")

"""### A partir da Intenção e Entidades --> Conectar com a Ontologia"""

#A ideia é fazer a consulta (intenção) usando os parâmetros (entidades) informadas...
#Também, aprimorar com o NLG abaixo onde a ontologia poderá fornecer o contexto

"""###Módulo 5: Geração de Resposta com IA Generativa (NLG)
O que é NLG?

Transforma informações estruturadas em frases naturais.
Exemplo:
Input: Pizza de pepperoni disponível nos tamanhos médio e grande.
Output: "Ótima escolha! A pizza de pepperoni está disponível nos tamanhos médio e grande."


"""

from transformers import pipeline

# 🔹 Criar pipeline de Question Answering (QA)
qa_pipeline = pipeline("question-answering", model="pierreguillou/bert-base-cased-squad-v1.1-portuguese")

# 🔹 Contexto com informações sobre livraria e categorias
contexto = """
O livro Capitães da Areia é um romance do autor Jorge Amado que pertence à categoria de literatura brasileira.
A editora Ipsilon que publica e disponibiliza os seus livros em uma livraria.
A livraria possui diferentes seções como literatura brasileira, literatura estrangeira, infantil e autoajuda.
Os livros de romance podem ser encontrados tanto na seção de literatura brasileira quanto na literatura estrangeira.
A seção de literatura brasileira contém obras de autores como Jorge Amado, Machado de Assis e Clarice Lispector.
A livraria oferece livros em diferentes formatos como capa dura, brochura e edição de bolso.
Os livros podem ser classificados por gêneros como romance, poesia, biografia e ficção científica.
"""

def gerar_resposta(intent, entities):
    if intent == "consulta_editora":
        pergunta = "Qual é o nome da editora?"
    elif intent == "consulta_autor":
        pergunta = f"Quem é o autor de {entities}?"
    elif intent == "consulta_categoria":
        pergunta = f"Qual é a categoria do livro {entities}?"
    else:
        return "Desculpe, não entendi a solicitação."

    print(f"Pergunta gerada: {pergunta}")
    resposta = qa_pipeline(question=pergunta, context=contexto)
    print(f"Resposta: {resposta['answer']}")  # Adicione esta linha
    return resposta["answer"]

# Teste com diferentes intenções
intencoes = ["consulta_autor", "consulta_categoria", "consulta_editora"]

for intencao in intencoes:
    print(f"\n=== Testando {intencao} ===")
    resposta = gerar_resposta(intencao, "Capitães da Areia")
    print(f"Resposta final: {resposta}")

!pip install ctransformers

from ctransformers import AutoModelForCausalLM
from typing import Dict, List, Optional
import torch

class BibliotecaLLM:
    def __init__(self):
        """
        Inicializa o componente LLM para a biblioteca usando LLaMA 2.
        Usa o modelo quantizado para economia de memória.
        """
        # Carrega o modelo LLaMA 2 quantizado
        self.model = AutoModelForCausalLM.from_pretrained(
            'TheBloke/Llama-2-7B-Chat-GGML',
            model_type='llama',
            gpu_layers=0  # Usar apenas CPU para compatibilidade
        )

        self.system_prompt = """
        Você é um assistente especializado em biblioteca que ajuda a responder perguntas sobre livros.
        Use as informações da ontologia para fornecer respostas precisas e naturais.
        Mantenha as respostas concisas e relevantes ao contexto da pergunta.
        """

    def prepare_context(self,
                       transcription: str,
                       intent: str,
                       entities: List[Dict],
                       ontology_data: Dict) -> str:
        """
        Prepara o contexto para o LLM com base nas informações extraídas.
        """
        context = f"""
        Pergunta do usuário: {transcription}
        Intenção identificada: {intent}

        Entidades encontradas:
        {self._format_entities(entities)}

        Informações da ontologia:
        {self._format_ontology(ontology_data)}

        Responda de forma natural e concisa.
        """
        return context

    def generate_response(self,
                         context: str,
                         max_tokens: int = 150) -> str:
        """
        Gera uma resposta usando o modelo LLaMA 2.
        """
        try:
            # Combina o prompt do sistema com o contexto
            full_prompt = f"{self.system_prompt}\n\n{context}"

            # Gera a resposta
            response = self.model(
                full_prompt,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.95,
                repetition_penalty=1.15
            )

            return response

        except Exception as e:
            print(f"Erro ao gerar resposta: {e}")
            return "Desculpe, não foi possível gerar uma resposta no momento."

    def _format_entities(self, entities: List[Dict]) -> str:
        """Formata as entidades para inclusão no contexto."""
        formatted = []
        for entity in entities:
            formatted.append(f"- {entity['text']} ({entity['type']})")
        return "\n".join(formatted)

    def _format_ontology(self, ontology_data: Dict) -> str:
        """Formata os dados da ontologia para inclusão no contexto."""
        formatted = []
        for key, value in ontology_data.items():
            if isinstance(value, list):
                formatted.append(f"{key}: {', '.join(value)}")
            else:
                formatted.append(f"{key}: {value}")
        return "\n".join(formatted)

# Exemplo de uso
if __name__ == "__main__":
    # Inicialização (não precisa de API key)
    llm = BibliotecaLLM()

    # Exemplo de dados
    transcription = "Qual o livro do autor Jorge Amado a editora Ipsilon publicou?"
    intent = "consulta_obra"
    entities = [
        {"text": "Jorge Amado", "type": "AUTOR"},
        {"text": "Ipsilon", "type": "EDITORA"}
    ]
    ontology_data = {
        "autor": "Jorge Amado",
        "obras": ["Capitães da Areia"],
        "editora": "Ipsilon"
    }

    # Preparar contexto e gerar resposta
    context = llm.prepare_context(transcription, intent, entities, ontology_data)
    response = llm.generate_response(context)
    print(f"Resposta: {response}")

"""TTS

###Modelo Baseado em Aprendizagem de Máquina
"""

# Instala a biblioteca gTTS (Google Text-to-Speech)
!pip install gtts

# Importa as bibliotecas necessárias
from gtts import gTTS  # Google TTS para conversão de texto em fala
from IPython.display import Audio  # Para reproduzir o áudio no Jupyter Notebook

# Define o texto que será sintetizado
text = response

# Cria o objeto TTS (define a língua do texto como português)
tts = gTTS(text, lang="pt")

# Salva o áudio gerado em um arquivo
output_file = "output_ml.mp3"  # Nome do arquivo de saída
tts.save(output_file)  # Salva o áudio gerado como MP3

print(f"Áudio gerado e salvo: '{output_file}'!")

from IPython.display import Audio


# Reproduzir áudio
Audio(output_file)